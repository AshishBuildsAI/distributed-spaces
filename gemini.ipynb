{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import os\n",
    "import google.generativeai as genai \n",
    "import tqdm as notebook_tqdm\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the language model only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is a question that has been pondered by philosophers and theologians for centuries, and there is no single, universally accepted answer. It's a deeply personal question, and what gives life meaning can vary greatly from person to person. \n",
       "> \n",
       "> Here are some perspectives:\n",
       "> \n",
       "> **Existentialist View:**  Life has no inherent meaning, and it is up to each individual to create their own purpose. This means finding meaning through their choices, actions, relationships, and experiences.\n",
       "> \n",
       "> **Nihilistic View:**  Life has no meaning or purpose whatsoever. This perspective can lead to feelings of despair and apathy.\n",
       "> \n",
       "> **Religious View:** Many religions believe that life has meaning and purpose given by a higher power. This purpose might involve serving God, achieving enlightenment, or living a virtuous life.\n",
       "> \n",
       "> **Humanistic View:**  Focuses on human potential, values, and the pursuit of happiness. Meaning is found through personal growth, contributing to society, and experiencing the richness of life.\n",
       "> \n",
       "> **Scientific View:**  From a scientific perspective, life's meaning is not a question that can be answered. The focus is on understanding the physical processes of life and the universe.\n",
       "> \n",
       "> **Other perspectives:** Some people find meaning in:\n",
       "> \n",
       "> * **Love and relationships:**  Building strong connections with others and experiencing love can be a powerful source of meaning.\n",
       "> * **Creativity and self-expression:**  Expressing oneself through art, music, writing, or other forms of creativity can be deeply fulfilling.\n",
       "> * **Work and contribution:**  Finding meaning in one's work, whether it's a job, a hobby, or volunteer work, can provide a sense of purpose.\n",
       "> * **Nature and the universe:**  Connecting with nature and appreciating the vastness of the cosmos can evoke a sense of wonder and awe.\n",
       "> \n",
       "> Ultimately, the meaning of life is a personal journey that each individual must undertake. There are no right or wrong answers, and what gives life meaning can change over time. The important thing is to actively seek out experiences and relationships that bring you joy, purpose, and fulfillment. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = language_model.generate_content(\"What is the meaning of life?\")\n",
    "to_markdown(response.text)\n",
    "#print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORT 5432\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"PORT\", os.environ[\"PORT\"])\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"ai\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=5432\n",
    ")\n",
    "register_vector(conn)\n",
    "dbschema = \"public\"\n",
    "dbtable = \"spaces_eicp\"\n",
    "cur = conn.cursor()\n",
    "model_dir = \"models/all-MiniLM-L6-v2\"\n",
    "main_model_dir = \"models/gemini-1.5-flash\"\n",
    "embedding_model =''\n",
    "language_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "#language_model.save(main_model_dir)\n",
    "\n",
    "# Check if the model directory exists\n",
    "if not os.path.exists(model_dir):\n",
    "    # Load and save the model to the specified directory\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embedding_model.save(model_dir)\n",
    "else:\n",
    "    # Load the model from the specified directory\n",
    "    embedding_model = SentenceTransformer(model_dir)\n",
    "\n",
    "def get_embeddings(content):\n",
    "    # Generate embeddings using the local model\n",
    "    embeddings = embedding_model.encode(content, convert_to_tensor=True)\n",
    "    return embeddings.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2.extras import execute_values\n",
    "import logging\n",
    "import json\n",
    "def get_top1_similar_docs(query_embedding, conn, schema, table):\n",
    "    # Normalize the query embedding\n",
    "    print(\"hello\")\n",
    "    query_embedding = np.array(query_embedding)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    # Use normalized embeddings and add additional filters for relevance\n",
    "    cur.execute(f\"\"\"\n",
    "        SELECT pageno, context, tabletext, source, imagepath, embedding, COALESCE(numtokens, 0)\n",
    "        FROM {schema}.{table}\n",
    "    \"\"\")\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    print(len(rows))\n",
    "    \n",
    "    # Compute similarities and store results\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        pageno, context, tabletext, source, imagepath, embedding_str, numtokens = row\n",
    "        try:\n",
    "            embedding = np.array(json.loads(embedding_str))  # Convert JSON string to numpy array\n",
    "            \n",
    "            # Safe normalization\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm == 0:\n",
    "                logging.warning(f\"Skipping normalization for zero vector: {embedding}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = embedding / norm\n",
    "            similarity = np.dot(query_embedding, embedding)\n",
    "            \n",
    "            # Ensure numtokens is a valid number\n",
    "            numtokens = numtokens if numtokens is not None else 0\n",
    "            \n",
    "            # Optionally, weight similarity by the number of tokens or other criteria\n",
    "            weighted_similarity = similarity * (1 + 0.01 * numtokens)\n",
    "            \n",
    "            results.append((pageno, context, tabletext, source, imagepath, weighted_similarity))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing row {row}: {e}\")\n",
    "    \n",
    "    # Sort results by weighted similarity\n",
    "    results.sort(key=lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    # Return the top 3 most similar documents\n",
    "    top3_docs = results[:5]\n",
    "    return top3_docs\n",
    "\n",
    "question = \"Explain the coefficients involved in Valve sizing calculations?\"\n",
    "embed = get_embeddings(question)\n",
    "try:\n",
    "    res = get_top1_similar_docs(query_embedding=embed, conn=conn,schema=dbschema, table=dbtable)\n",
    "except Exception as e:\n",
    "    print(\"Exception: \", e)\n",
    "\n",
    "#envelope=f\"You are a friendly AI assitant who finds information for HR assistants, Engineers, sales teams and many more ... only using the given context {res} answer the question {question}\"\n",
    "#response = language_model.generate_content(envelope)\n",
    "#print(response.text)\n",
    "#to_markdown(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "embed = ollama.embeddings(model='nomic-embed-text', prompt='This is example text')\n",
    "print(len(embed['embedding']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
